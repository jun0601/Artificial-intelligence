{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TJF4tEn2EoS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "594cfb50-25ee-4d81-cc35-3ddd46ebc628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Q table is\n",
            "\n",
            "col=0       1        2        3       4         5\n",
            "row: 0\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 1\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 2\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 3\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 4\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 5\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 6\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 7\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 8\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "\n",
            "Learning starts.\n",
            "\n",
            "episode= 0   epsilon= 0.4\n",
            "episode= 5000   epsilon= 0.36924653855465434\n",
            "episode= 10000   epsilon= 0.3408575155864846\n",
            "episode= 15000   epsilon= 0.3146511444266214\n",
            "episode= 20000   epsilon= 0.2904596148294764\n",
            "episode= 25000   epsilon= 0.26812801841425576\n",
            "episode= 30000   epsilon= 0.24751335672245633\n",
            "episode= 35000   epsilon= 0.22848362553952595\n",
            "episode= 40000   epsilon= 0.21091696961721942\n",
            "episode= 45000   epsilon= 0.19470090238398868\n",
            "\n",
            "Learning is finished. the Q table is:\n",
            "\n",
            "col=0       1        2        3       4         5\n",
            "row: 0\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 1\n",
            " 0.00,   -9.00,   -9.00,   -9.00,   -9.00,   -9.00,   -9.00,   -9.00,    0.00\n",
            " 0.00,    4.30,    4.78,    5.31,    5.90,    5.31,    4.78,   -9.00,    0.00\n",
            " 0.00,    4.30,    4.78,   -9.00,   -9.00,    6.56,   -9.00,    5.31,    0.00\n",
            " 0.00,   -9.00,    3.87,    4.30,    4.78,    5.31,    5.90,    5.31,    0.00\n",
            "row: 2\n",
            " 0.00,    3.87,    4.30,    0.00,    0.00,    5.90,    0.00,    0.00,    0.00\n",
            " 0.00,    4.78,   -9.00,    0.00,    0.00,   -9.00,    0.00,   -9.00,    0.00\n",
            " 0.00,    4.78,    5.31,    0.00,    0.00,    7.29,    0.00,    5.90,    0.00\n",
            " 0.00,   -9.00,    4.30,    0.00,    0.00,   -9.00,    0.00,   -9.00,    0.00\n",
            "row: 3\n",
            " 0.00,    4.30,    4.78,   -9.00,   -9.00,    6.56,   -9.00,    5.31,    0.00\n",
            " 0.00,    5.31,    5.90,    6.56,    7.29,    6.56,    5.90,   -9.00,    0.00\n",
            " 0.00,    4.30,   -9.00,    5.31,   -9.00,    8.10,    7.29,   -9.00,    0.00\n",
            " 0.00,   -9.00,    4.78,    5.31,    5.90,    6.56,    7.29,    6.56,    0.00\n",
            "row: 4\n",
            " 0.00,    4.78,    0.00,    5.90,    0.00,    7.29,    6.56,    0.00,    0.00\n",
            " 0.00,   -9.00,    0.00,   -9.00,    0.00,    7.29,   -9.00,    0.00,    0.00\n",
            " 0.00,    3.87,    0.00,    4.78,    0.00,    9.00,    8.10,    0.00,    0.00\n",
            " 0.00,   -9.00,    0.00,   -9.00,    0.00,   -9.00,    8.10,    0.00,    0.00\n",
            "row: 5\n",
            " 0.00,    4.30,   -9.00,    5.31,   -9.00,    0.00,    7.29,   -9.00,    0.00\n",
            " 0.00,    4.30,    4.78,    4.30,    0.00,    0.00,    7.29,   -9.00,    0.00\n",
            " 0.00,    3.49,   -9.00,   -9.00,    0.00,    0.00,    7.29,    6.56,    0.00\n",
            " 0.00,   -9.00,    3.87,    4.30,    4.78,    0.00,    9.00,    8.10,    0.00\n",
            "row: 6\n",
            " 0.00,    3.87,    0.00,    0.00,    4.30,    0.00,    8.10,    7.29,    0.00\n",
            " 0.00,   -9.00,    0.00,    0.00,   -9.00,    0.00,    6.56,   -9.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    6.56,    0.00,    0.00\n",
            " 0.00,   -9.00,    0.00,    0.00,   -9.00,    0.00,   -9.00,    7.29,    0.00\n",
            "row: 7\n",
            " 0.00,    0.00,    0.00,   -9.00,    0.00,    0.00,    7.29,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,   -9.00,    0.00\n",
            " 0.00,   -9.00,    0.00,    0.00,    0.00,    0.00,    0.00,   -9.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 8\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "\n",
            "Test starts.\n",
            "\n",
            "\n",
            "Episode: 0  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: right  that leads to state ( 1 ,  3 )\n",
            "The move is: right  that leads to state ( 1 ,  4 )\n",
            "The move is: right  that leads to state ( 1 ,  5 )\n",
            "The move is: down   that leads to state ( 2 ,  5 )\n",
            "The move is: down   that leads to state ( 3 ,  5 )\n",
            "The move is: down   that leads to state ( 4 ,  5 )\n",
            "The move is: down   that leads to state ( 5 ,  5 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 1  start state: ( 1 ,  1 )\n",
            "The move is: down   that leads to state ( 2 ,  1 )\n",
            "The move is: down   that leads to state ( 3 ,  1 )\n",
            "The move is: right  that leads to state ( 3 ,  2 )\n",
            "The move is: right  that leads to state ( 3 ,  3 )\n",
            "The move is: right  that leads to state ( 3 ,  4 )\n",
            "The move is: right  that leads to state ( 3 ,  5 )\n",
            "The move is: down   that leads to state ( 4 ,  5 )\n",
            "The move is: down   that leads to state ( 5 ,  5 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 2  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: down   that leads to state ( 2 ,  2 )\n",
            "The move is: down   that leads to state ( 3 ,  2 )\n",
            "The move is: right  that leads to state ( 3 ,  3 )\n",
            "The move is: right  that leads to state ( 3 ,  4 )\n",
            "The move is: right  that leads to state ( 3 ,  5 )\n",
            "The move is: down   that leads to state ( 4 ,  5 )\n",
            "The move is: down   that leads to state ( 5 ,  5 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 3  start state: ( 1 ,  1 )\n",
            "The move is: down   that leads to state ( 2 ,  1 )\n",
            "The move is: right  that leads to state ( 2 ,  2 )\n",
            "The move is: down   that leads to state ( 3 ,  2 )\n",
            "The move is: right  that leads to state ( 3 ,  3 )\n",
            "The move is: right  that leads to state ( 3 ,  4 )\n",
            "The move is: right  that leads to state ( 3 ,  5 )\n",
            "The move is: down   that leads to state ( 4 ,  5 )\n",
            "The move is: down   that leads to state ( 5 ,  5 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 4  start state: ( 1 ,  1 )\n",
            "The move is: down   that leads to state ( 2 ,  1 )\n",
            "The move is: right  that leads to state ( 2 ,  2 )\n",
            "The move is: down   that leads to state ( 3 ,  2 )\n",
            "The move is: right  that leads to state ( 3 ,  3 )\n",
            "The move is: right  that leads to state ( 3 ,  4 )\n",
            "The move is: right  that leads to state ( 3 ,  5 )\n",
            "The move is: down   that leads to state ( 4 ,  5 )\n",
            "The move is: down   that leads to state ( 5 ,  5 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "Program ends!!!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "total_episodes = 50000       # Total episodes\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.9                 # Discounting rate\n",
        "alpha = 1.0\t\t\t\t\t # update parameter\n",
        "\n",
        "# Exploration parameters\n",
        "original_epsilon = 0.4           # Exploration rate\n",
        "decay_rate = 0.000016            # Exponential decay rate for exploration prob\n",
        "random.seed(datetime.now().timestamp())\t# give a new seed in random number generation.\n",
        "\n",
        "# state space is defined as size_row X size_col array.\n",
        "# The boundary cells are holes(H).\n",
        "# S: start, G: goal, H:hole, F:frozen\n",
        "\n",
        "max_row = 9\n",
        "max_col = 9\n",
        "max_num_actions = 4\n",
        "\n",
        "env_state_space = [\n",
        "    ['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H'],\n",
        "    ['H', 'S', 'F', 'F', 'F', 'F', 'F', 'F', 'H'],\n",
        "    ['H', 'F', 'F', 'H', 'H', 'F', 'H', 'F', 'H'],\n",
        "    ['H', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'H'],\n",
        "    ['H', 'F', 'H', 'F', 'H', 'F', 'F', 'H', 'H'],\n",
        "    ['H', 'F', 'F', 'F', 'F', 'G', 'F', 'F', 'H'],\n",
        "    ['H', 'F', 'H', 'H', 'F', 'H', 'F', 'F', 'H'],\n",
        "    ['H', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'H'],\n",
        "    ['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H']\n",
        "]\n",
        "\n",
        "# Create our Q table and initialize its value.\n",
        "#   dim0:row, dim1:column, dim2: action.\n",
        "# Q-table is initialized as 0.0.\n",
        "# for terminal states(H or G), q-a value should be always 0.\n",
        "\n",
        "Q = np.zeros((max_row,\tmax_col,  max_num_actions))\n",
        "\n",
        "# offset of each move action:  up, right, down, left, respectively.\n",
        "# a new state(location) = current state + offset of an action.\n",
        "#        action = up    right  down    left.\n",
        "move_offset = [[-1,0], [0,1],   [1,0],  [0,-1]]\n",
        "move_str =\t   ['up   ', 'right', 'down ', 'left ']\n",
        "\n",
        "def display_Q_table (Q):\n",
        "\tprint(\"\\ncol=0       1        2        3       4         5\")\n",
        "\tfor r in range(max_row):\n",
        "\t\tprint(\"row:\", r)\n",
        "\t\tfor a in range(max_num_actions):\n",
        "\t\t\tfor c in range(max_col):\n",
        "\t\t\t\ttext = \"{:5.2f}\".format(Q[r,c,a])\n",
        "\t\t\t\tif c == 0:\n",
        "\t\t\t\t\tline = text\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tline = line + \",   \" + text\n",
        "\t\t\tprint(line)\n",
        "\n",
        "def my_argmax(q_a_list):\n",
        "    mv = max(q_a_list)\n",
        "    m_positions = [i for i, q in enumerate(q_a_list) if q == mv]\n",
        "    nmax = len(m_positions)\n",
        "    thresh = np.linspace(1 / nmax, 1, nmax)\n",
        "    r_n = random.random()\n",
        "\n",
        "    for i, t in enumerate(thresh):\n",
        "        if r_n <= t:\n",
        "            return m_positions[i]\n",
        "\n",
        "    return m_positions[-1]\n",
        "\n",
        "# choose an action with epsilon-greedy approach according to Q.\n",
        "# return value: an action index(0 ~ 3)\n",
        "def choose_action_with_epsilon_greedy(s, epsilon):\n",
        "\tr = s[0]\n",
        "\tc = s[1]\n",
        "\t# get q-a values of all actions of current state.\n",
        "\tq_a_list = Q[r,c,:]\n",
        "\tmax = my_argmax(q_a_list)\t# max is action with biggest q-a value.\n",
        "\trn = random.random()\t# 0~1 사이의 random 실수 생성.\n",
        "\n",
        "\tif rn >= epsilon:\t# epsilon 보다 크면, action max is selected.\n",
        "\t\taction = max\n",
        "\telse:\n",
        "\t\trn1 = random.random()\n",
        "\t\t# 4 개의 action 중 하나를 무작위로 선택.\n",
        "\t\tif rn1 >= 0.75:\n",
        "\t\t\taction = 0\n",
        "\t\telif rn1>= 0.5:\n",
        "\t\t\taction = 1\n",
        "\t\telif rn1 >= 0.25:\n",
        "\t\t\taction = 2\n",
        "\t\telse:\n",
        "\t\t\taction = 3\n",
        "\treturn action\n",
        "\n",
        "# Q 가 가진 policy 를 greedy 하게 적용하여 취할 action 을 고른다.\n",
        "def choose_action_with_greedy(s):\n",
        "\tr = s[0]\n",
        "\tc = s[1]\n",
        "\t# get q-a values of all actions of state s.\n",
        "\tq_a_list = Q[r,c,:]\n",
        "\tmax_action = my_argmax(q_a_list)\t# max is action with biggest q-a value.\n",
        "\treturn max_action\n",
        "\n",
        "# get new state and reward for taking action a at state s.\n",
        "# deterministic movement is taken.\n",
        "# reward is given as: F/S:0;  H:-5;   G:5.\n",
        "def get_new_state_and_reward(s, a):\n",
        "\tnew_state = []\n",
        "\toff_set = move_offset[a]\n",
        "\n",
        "\t#  s + off_set gives the new_state.\n",
        "\tnew_state.append(s[0] + off_set[0])\n",
        "\tnew_state.append(s[1] + off_set[1])\n",
        "\n",
        "\t# compute reward for moving to the new state\n",
        "\tcell = env_state_space[new_state[0]][new_state[1]]\n",
        "\tif cell == 'F':\n",
        "\t\trew = 0\n",
        "\telif cell == 'H':\n",
        "\t\trew = -9\n",
        "\telif cell == 'G':\n",
        "\t\trew = 9\n",
        "\telif cell == 'S':\n",
        "\t\trew = 0\n",
        "\telse:\n",
        "\t\tprint(\"Logic error in get_new_state_and_reward. This cannot happen!\")\n",
        "\t\ttime.sleep(1200)\n",
        "\t\treturn [0,0], -20000\n",
        "\treturn new_state, rew\n",
        "\n",
        "# Environment 출력: agent 가 있는 곳에는 * 로 표시.\n",
        "# agent 의 현재 위치(즉 current state): s\n",
        "def env_rendering(s):\n",
        "\tfor i in range(0, max_row, 1):\n",
        "\t\tline = ''\n",
        "\t\tfor j in range(0, max_col, 1):\n",
        "\t\t\tline = line + env_state_space[i][j]\n",
        "\t\tif s[0] == i:\n",
        "\t\t\tcol = s[1]\n",
        "\t\t\tline1 = line[:col] + '*' +line[col+1:]\n",
        "\t\telse:\n",
        "\t\t\tline1 = line\n",
        "\t\tprint(line1)\n",
        "\n",
        "# Learning stage: it iterates for an huge number of episodes\n",
        "print(\"Initial Q table is\")\n",
        "display_Q_table(Q)\n",
        "\n",
        "# start state is the cell with 'S'. terminal states are those with 'H' or 'G.\n",
        "start_state = [1,1]\n",
        "\n",
        "print(\"\\nLearning starts.\\n\")\n",
        "for episode in range(total_episodes):\n",
        "\t# set the start state of an episode.\n",
        "\tS = start_state\n",
        "\n",
        "\t# we use decayed epsilon in exploration so that it decreases as time goes on.\n",
        "\tepsilon = original_epsilon * math.exp(-decay_rate*episode)\n",
        "\n",
        "\tif episode % 5000 == 0:\n",
        "\t\tprint('episode=', episode, '  epsilon=', epsilon)\n",
        "\t\ttime.sleep(1)\n",
        "\n",
        "\tfor step in range(max_steps):\n",
        "\t\t# Choose an action A from S using policy derived from Q with epsilon-greedy.\n",
        "\t\tA = choose_action_with_epsilon_greedy(S, epsilon)\n",
        "\n",
        "\t\t# take action A to observe reward R, and new state S_.\n",
        "\t\tS_ , R = get_new_state_and_reward(S, A)\n",
        "\n",
        "\t\tr=S[0]\t# row of S\n",
        "\t\tc=S[1]\t# column of S\n",
        "\n",
        "\t\t# update state-action value Q(s,a)\n",
        "\t\tQ[r][c][A] = Q[r][c][A] + alpha * (R + gamma * np.max(Q[S_[0]][S_[1]][:]) - Q[r][c][A] )\n",
        "\n",
        "\t\tS = S_\t# move to the new state.\n",
        "\n",
        "\t\t# if the new state S is a terminal state, terminate the episode.\n",
        "\t\tif env_state_space[S[0]][S[1]] == 'G' or env_state_space[S[0]][S[1]] == 'H':\n",
        "\t\t\tbreak\n",
        "\n",
        "print('\\nLearning is finished. the Q table is:')\n",
        "display_Q_table(Q)\n",
        "# time.sleep(600)\n",
        "\n",
        "# Test stage: agent 가 길을 찾아 가는 실험.\n",
        "\n",
        "print(\"\\nTest starts.\\n\")\n",
        "\n",
        "for e in range(5):\n",
        "\tS = start_state\n",
        "\ttotal_rewards = 0\n",
        "\tprint(\"\\nEpisode:\", e, \" start state: (\", S[0], \", \", S[1], \")\")\n",
        "\n",
        "\tfor step in range(max_steps):\n",
        "\n",
        "\t\tA = choose_action_with_greedy(S)\t\t# S 에서 greedy 하게 다음 action 을 선택.\n",
        "\n",
        "\t\tS_ , R = get_new_state_and_reward(S, A)\n",
        "\n",
        "\t\tprint(\"The move is:\", move_str[A], \" that leads to state (\", S_[0], \", \", S_[1], \")\" )\n",
        "\n",
        "\t\ttotal_rewards += R\n",
        "\t\tS = S_\n",
        "\t\tif env_state_space[S[0]][S[1]] == 'G' or env_state_space[S[0]][S[1]] == 'H':\n",
        "\n",
        "\t\t\tbreak\n",
        "\tprint(\"Episode has ended. Total reward received in episode = \", total_rewards)\n",
        "\ttime.sleep(1)\n",
        "\n",
        "print(\"Program ends!!!\")\n",
        "\n"
      ]
    }
  ]
}